{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Spark\n",
    "\n",
    "Whenever we work in Spark the first thing we need is the spark contect (sc).  We are going to use the module `findspark` to get access to the spark context.  First we need to install the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): findspark in /home/csumb/anaconda2/lib/python2.7/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "! pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we specify the path to spark - which for us is on the local VM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init(os.getenv('HOME') + '/spark-1.6.0-bin-hadoop2.6')\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import pyspark and get the spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f6df43ec290>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "try: \n",
    "    print(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext()\n",
    "    print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an RDD\n",
    "\n",
    "From the Spark documentation:\n",
    "\n",
    "_\"A Resilient Distributed Dataset (RDD), the basic abstraction in Spark, represents an immutable, partitioned collection of elements that can be operated on in parallel.\"_\n",
    "\n",
    "_\"Parallelized collections are created by calling SparkContextâ€™s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel.\"_ \n",
    "\n",
    "For example, here is how to create a parallelized collection holding the numbers 1 to 5:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[52] at parallelize at PythonRDD.scala:423\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "print(distData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD exists in the Spark Context which may or may not be in the notebook kernel.\n",
    "\n",
    "We apply transformations and actions to the RDD. The RDD will execute operations in parallel, for example to add up elements of list.\n",
    "\n",
    "Spark is heavily functional (built in Scala).  For example, map, reduce and filter operations are supported - these functions take functions or lambda functions as arguments: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics - transformations and actions\n",
    "\n",
    "The RDD is not loaded in memory - it is just a pointer to the file.  Spark allows us to apply transformations to the RDD - but these are computed immediately - Spark is intentionally lazy.  Nothing is computed until we execute an action, at which point the Spark driver creates tasks to run on separate nodes in the Spark cluster.  Each node executes the transformations and actions and returns the results to the driver.   \n",
    "\n",
    "http://vishnuviswanath.com/spark_rdd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData = sc.parallelize(data) \\\n",
    "                .filter(lambda x : x > 3)\n",
    "distData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 25]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData = sc.parallelize(data) \\\n",
    "                .filter(lambda x : x > 3) \\\n",
    "                .map(lambda x : x ** 2)\n",
    "distData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions collect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add, mul \n",
    "\n",
    "distData = sc.parallelize(data) \\\n",
    "                .filter(lambda x : x > 3) \\\n",
    "                .map(lambda x : x ** 2) \\\n",
    "                .reduce(add)\n",
    "\n",
    "distData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Sum : ' + distData.reduce(mul))\n",
    "distData.reduce(mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Data Sources\n",
    "\n",
    "We can also create RDDs from external data sources such as Hadoop, Amazon S3 and files. Here we will create a text file RDD.  NOte that we must use absolute paths since this code is pushed onto the Spark cluster - it is not run in the context of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[72] at textFile at NativeMethodAccessorImpl.java:-2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'2,\"ZIPP VUKA CARBON AERO BASE BAR AND EXTENSIONS COMPLETE TRIATHLON TT TRI CYCLING\"',\n",
       " u'3,\"Cycling Bicycle MTB Bike Fixie Gloss 3K Carbon Fiber Riser Bar Handlebar 31.8mm\"',\n",
       " u'4,\"BICYCLE RIMS 26\"\"x 50MM RED 3 SPEED INTERNAL HUB WHEEL SET BEACH CRUISER BIKE\"',\n",
       " u'5,\"Mavic Crossride 26\"\" Mountain bike wheels and WTB Weirwolf Tires\"',\n",
       " u'6,\"New KCNC ARROW 7050 Alloy Stem ',\n",
       " u'7,\"ROTOR QXL Aero Oval Road Chainring BCD110x5 53t\"',\n",
       " u'8,\"Yakima 4 pack SKS lock cores & 2 keys - A142 - roof rack locking cylinders\"',\n",
       " u'9,\"Sram Force Carbon Crank Gxp 110 Bcd No Chainrings 175 mm (2700)\"',\n",
       " u'10,\"THE ORIGINAL SQUIRT LONG LASTING DRY CHAIN BICYCLE LUBE WAX BASED\"',\n",
       " u'11,\"BV Bike Rear Saddle Bag Cycling Seat Post Pouch Bicycle Tail Storage NEW SB1-L\"']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('/home/csumb/data-science-for-search/data/bike-item-titles.txt')\n",
    "print(rdd)\n",
    "\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Words\n",
    "\n",
    "To illustrate RDD basics, consider the simple program below which counts the number of words in the text file rdd we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9894\n"
     ]
    }
   ],
   "source": [
    "words_per_line = rdd.map(lambda s: len(s[0].split()))\n",
    "#words_per_line.take(100)\n",
    "total_words = words_per_line.reduce(add)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reiterate - `words_per_line` is applies a transformation to the rdd - it is lazy and not evaluated until we apply an action - such as `reduce()`.  We can inspect the transformations applied to the RDD using the `toDebugString()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) PythonRDD[78] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[72] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      " |  /home/csumb/data-science-for-search/data/bike-item-titles.txt HadoopRDD[71] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print(words_per_line.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency in Spark\n",
    "\n",
    "Many ways to do this - here are two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "terms1 = rdd.flatMap(lambda s : s.split(' ')) \\\n",
    "            .countByValue()\n",
    "\n",
    "terms2 = rdd.flatMap(lambda s : s.split()) \\\n",
    "            .map(lambda w : (w, 1)) \\\n",
    "            .reduceByKey(lambda x,y : x+y) \\\n",
    "            .collectAsMap()\n",
    "\n",
    "print(terms1['bike'])\n",
    "print(terms2['bike'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrames API\n",
    "\n",
    "If you look carefully above the text file is represented as a CSV and we did not parse the lines correctly.  CSV parsing is complex - but is made easier using Spark Data Frames which is an abstraction on top of RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          item_title|\n",
      "+---+--------------------+\n",
      "|  2|ZIPP VUKA CARBON ...|\n",
      "|  3|Cycling Bicycle M...|\n",
      "|  4|BICYCLE RIMS 26\"x...|\n",
      "|  5|Mavic Crossride 2...|\n",
      "|  7|ROTOR QXL Aero Ov...|\n",
      "|  8|Yakima 4 pack SKS...|\n",
      "|  9|Sram Force Carbon...|\n",
      "| 10|THE ORIGINAL SQUI...|\n",
      "| 11|BV Bike Rear Sadd...|\n",
      "| 12|HELIX BMX ROUND D...|\n",
      "| 13|Waterproof Bicycl...|\n",
      "| 14|Brand New CycleOp...|\n",
      "| 15|Planet Bike LED S...|\n",
      "| 16|Bike Bicycle Head...|\n",
      "| 17|New Helmet Teenag...|\n",
      "| 18|2 Pcs Bike Roller...|\n",
      "| 19|FSA BICYCLE COMPR...|\n",
      "| 20|Kenda Tube 26 X1....|\n",
      "| 21|Bicycle Lock Set ...|\n",
      "| 22|NEW DT Swiss 350 ...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "        .options(header='false', inferSchema='true') \\\n",
    "        .load('/home/csumb/data-science-for-search/data/bike-item-titles.txt') \\\n",
    "        .selectExpr(\"C0 as id\",\"C1 as item_title\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,IntegerType,true),StructField(item_title,StringType,true)))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames API has functional model that can be applied to data frame objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, item_title: string]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['id'] >=5).filter(df['id'] <= 10)\n",
    "\n",
    "#df.filter(df['id'] >=5).filter(df['id'] <= 10).count()\n",
    "\n",
    "#df.filter(df['id'] >=5).filter(df['id'] <= 10).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also has SQL interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          item_title|\n",
      "+---+--------------------+\n",
      "|  5|Mavic Crossride 2...|\n",
      "|  7|ROTOR QXL Aero Ov...|\n",
      "|  8|Yakima 4 pack SKS...|\n",
      "|  9|Sram Force Carbon...|\n",
      "| 10|THE ORIGINAL SQUI...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.registerDataFrameAsTable(df,'bikeitems')\n",
    "sqlContext.tableNames()\n",
    "\n",
    "sqlContext.sql(\"select id, item_title from bikeitems where id between 5 and 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert back and forth RDD <> DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=2, item_title=u'ZIPP VUKA CARBON AERO BASE BAR AND EXTENSIONS COMPLETE TRIATHLON TT TRI CYCLING'),\n",
       " Row(id=3, item_title=u'Cycling Bicycle MTB Bike Fixie Gloss 3K Carbon Fiber Riser Bar Handlebar 31.8mm'),\n",
       " Row(id=4, item_title=u'BICYCLE RIMS 26\"x 50MM RED 3 SPEED INTERNAL HUB WHEEL SET BEACH CRUISER BIKE'),\n",
       " Row(id=5, item_title=u'Mavic Crossride 26\" Mountain bike wheels and WTB Weirwolf Tires'),\n",
       " Row(id=7, item_title=u'ROTOR QXL Aero Oval Road Chainring BCD110x5 53t'),\n",
       " Row(id=8, item_title=u'Yakima 4 pack SKS lock cores & 2 keys - A142 - roof rack locking cylinders'),\n",
       " Row(id=9, item_title=u'Sram Force Carbon Crank Gxp 110 Bcd No Chainrings 175 mm (2700)'),\n",
       " Row(id=10, item_title=u'THE ORIGINAL SQUIRT LONG LASTING DRY CHAIN BICYCLE LUBE WAX BASED'),\n",
       " Row(id=11, item_title=u'BV Bike Rear Saddle Bag Cycling Seat Post Pouch Bicycle Tail Storage NEW SB1-L'),\n",
       " Row(id=12, item_title=u'HELIX BMX ROUND DROPOUT SAVERS -FITS NEARLY ALL FRAMES -Fits 3/8\" AND 10mm Axles')]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df.rdd\n",
    "df2 = rdd.toDF()\n",
    "\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ZIPP', 2),\n",
       " (u'VUKA', 2),\n",
       " (u'CARBON', 2),\n",
       " (u'AERO', 2),\n",
       " (u'BASE', 2),\n",
       " (u'BAR', 2),\n",
       " (u'AND', 2),\n",
       " (u'EXTENSIONS', 2),\n",
       " (u'COMPLETE', 2),\n",
       " (u'TRIATHLON', 2)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = rdd.flatMap(lambda row : [ ( word, row[0]) for word in row[1].split(' ') ] ) \n",
    "index.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', <pyspark.resultiterable.ResultIterable at 0x7f6df2a4d6d0>),\n",
       " (u'Powerlock-New', <pyspark.resultiterable.ResultIterable at 0x7f6df2a4d790>),\n",
       " (u'BLACK/SILVER', <pyspark.resultiterable.ResultIterable at 0x7f6df2a4df90>),\n",
       " (u'SecurityIng', <pyspark.resultiterable.ResultIterable at 0x7f6df2a4d050>),\n",
       " (u'SporstWear', <pyspark.resultiterable.ResultIterable at 0x7f6df2a4dc50>),\n",
       " (u'(28.6)', <pyspark.resultiterable.ResultIterable at 0x7f6df2a4de90>),\n",
       " (u'S-5', <pyspark.resultiterable.ResultIterable at 0x7f6df2a4dad0>),\n",
       " (u'Interloc', <pyspark.resultiterable.ResultIterable at 0x7f6df2a4d1d0>),\n",
       " (u'S-2', <pyspark.resultiterable.ResultIterable at 0x7f6df2a4d910>),\n",
       " (u'yellow', <pyspark.resultiterable.ResultIterable at 0x7f6df2a4d710>)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = rdd.flatMap(lambda row : [ (word,  row[0]) for word in row[1].split(' ') ] ) \\\n",
    "            .groupByKey()\n",
    "index.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Unicycle', [2138, 3748, 7232, 8777])]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = rdd.flatMap(lambda row : [ (word,  row[0]) for word in row[1].split(' ') ] ) \\\n",
    "            .groupByKey() \\\n",
    "            .map(lambda x : (x[0], list(x[1])))\n",
    "index.filter(lambda x : x[0] == 'Unicycle').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = rdd.flatMap(lambda row : [ (word,  row[0]) for word in row[1].split(' ') ] ) \\\n",
    "            .groupByKey() \\\n",
    "            .map(lambda x : (x[0], list(x[1]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Helmet',\n",
       "  [17,\n",
       "   53,\n",
       "   65,\n",
       "   102,\n",
       "   217,\n",
       "   262,\n",
       "   502,\n",
       "   587,\n",
       "   605,\n",
       "   657,\n",
       "   671,\n",
       "   698,\n",
       "   743,\n",
       "   753,\n",
       "   895,\n",
       "   938,\n",
       "   945,\n",
       "   1077,\n",
       "   1299,\n",
       "   1460,\n",
       "   1508,\n",
       "   1651,\n",
       "   1764,\n",
       "   1897,\n",
       "   1951,\n",
       "   2132,\n",
       "   2178,\n",
       "   2368,\n",
       "   2423,\n",
       "   2434,\n",
       "   2458,\n",
       "   2514,\n",
       "   2552,\n",
       "   2630,\n",
       "   2692,\n",
       "   2825,\n",
       "   2975,\n",
       "   3079,\n",
       "   3151,\n",
       "   3189,\n",
       "   3233,\n",
       "   3233,\n",
       "   3234,\n",
       "   3265,\n",
       "   3320,\n",
       "   3484,\n",
       "   3484,\n",
       "   3485,\n",
       "   3485,\n",
       "   3667,\n",
       "   3687,\n",
       "   3990,\n",
       "   4094,\n",
       "   4094,\n",
       "   4259,\n",
       "   4681,\n",
       "   4769,\n",
       "   4808,\n",
       "   4849,\n",
       "   4899,\n",
       "   5227,\n",
       "   5241,\n",
       "   5299,\n",
       "   5478,\n",
       "   5485,\n",
       "   5550,\n",
       "   5558,\n",
       "   5577,\n",
       "   5577,\n",
       "   5599,\n",
       "   5736,\n",
       "   5803,\n",
       "   5822,\n",
       "   5890,\n",
       "   6047,\n",
       "   6047,\n",
       "   6371,\n",
       "   6375,\n",
       "   6386,\n",
       "   6386,\n",
       "   6414,\n",
       "   6418,\n",
       "   6449,\n",
       "   6549,\n",
       "   6690,\n",
       "   6725,\n",
       "   6756,\n",
       "   6940,\n",
       "   7020,\n",
       "   7032,\n",
       "   7099,\n",
       "   7190,\n",
       "   7539,\n",
       "   7721,\n",
       "   7747,\n",
       "   7757,\n",
       "   7847,\n",
       "   8055,\n",
       "   8193,\n",
       "   8239,\n",
       "   8521,\n",
       "   8664,\n",
       "   8768,\n",
       "   8793,\n",
       "   9054,\n",
       "   9137,\n",
       "   9137,\n",
       "   9428,\n",
       "   9624,\n",
       "   9753])]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.filter(lambda x : x[0] == 'Helmet').collect()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
