{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Spark\n",
    "\n",
    "Whenever we work in Spark the first thing we need is the spark contect (sc).  We are going to use the module `findspark` to get access to the spark context.  First we need to install the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): findspark in /home/csumb/anaconda2/lib/python2.7/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "! pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we specify the path to spark - which for us is on the local VM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init('/home/csumb/spark-1.6.0-bin-hadoop2.6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import pyspark and get the spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7fc2d893d250>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "try: \n",
    "    print(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext()\n",
    "    print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an RDD\n",
    "\n",
    "From the Spark documentation:\n",
    "\n",
    "_\"A Resilient Distributed Dataset (RDD), the basic abstraction in Spark, represents an immutable, partitioned collection of elements that can be operated on in parallel.\"_\n",
    "\n",
    "_\"Parallelized collections are created by calling SparkContextâ€™s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel.\"_ \n",
    "\n",
    "For example, here is how to create a parallelized collection holding the numbers 1 to 5:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD will now execute operations in parallel, for example to add up elements of list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Data Sources\n",
    "\n",
    "We can also create RDDs from external data sources such as Hadoop, Amazon S3 and files. Here we will create a text file RDD.  NOte that we must use absolute paths since this code is pushed onto the Spark cluster - it is not run in the context of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[19] at textFile at NativeMethodAccessorImpl.java:-2\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile('/home/csumb/data-science-for-search/data/bike-item-titles.txt')\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics - transformations and actions\n",
    "\n",
    "The RDD is not loaded in memory - it is just a pointer to the file.  Spark allows us to apply transformations to the RDD - but these are computed immediately - Spark is intentionally lazy.  Nothing is computed until we execute an action, at which point the Spark driver creates tasks to run on separate nodes in the Spark cluster.  Each node executes the transformations and actions and returns the results to the driver.   \n",
    "\n",
    "http://vishnuviswanath.com/spark_rdd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Words\n",
    "\n",
    "To illustrate RDD basics, consider the simple program below which counts the number of words in the text file rdd we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[22] at RDD at PythonRDD.scala:43\n",
      "106983\n"
     ]
    }
   ],
   "source": [
    "words_per_line = rdd.map(lambda s: len(s.split()))\n",
    "# print(words_per_line)\n",
    "total_words = words_per_line.reduce(lambda a, b: a + b)\n",
    "# print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 13, 14, 10, 6, 8, 16, 12, 11, 14]\n"
     ]
    }
   ],
   "source": [
    "print(words_per_line.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reiterate - `words_per_line` is applies a transformation to the rdd - it is lazy and not evaluated until we apply an action - such as `reduce()`.  We can inspect the transformations applied to the RDD using the `toDebugString()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) PythonRDD[22] at RDD at PythonRDD.scala:43 [Memory Serialized 1x Replicated]\n",
      " |       CachedPartitions: 1; MemorySize: 10.0 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      " |  MapPartitionsRDD[19] at textFile at NativeMethodAccessorImpl.java:-2 [Memory Serialized 1x Replicated]\n",
      " |  /home/csumb/data-science-for-search/data/bike-item-titles.txt HadoopRDD[18] at textFile at NativeMethodAccessorImpl.java:-2 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "print(words_per_line.toDebugString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
