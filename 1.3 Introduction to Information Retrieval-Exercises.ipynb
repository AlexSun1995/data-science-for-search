{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Introduction to Information Retrieval\n",
    "\n",
    "Here we work with a data set scraped from eBay.  The data contains 9895 item titles and descriptions.\n",
    "\n",
    "First we load the data file and _normalise_ the text - removing certain characters and converting to lower case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook, show, vplot\n",
    "from bokeh.charts import Bar, Scatter, BoxPlot\n",
    "from bokeh.charts.attributes import CatAttr\n",
    "from bokeh.models import ColumnDataSource\n",
    "\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cycling bicycle mtb bike fixie gloss carbon fiber riser bar handlebar', 'description feature easy to use made of high quality carbon fiber with the special design can save for a long time the carbon fiber handlebar is made of high quality carbon fiber so that you can use it relieved this quick disassembling carbon fiber handlebar is easy to use and one of the best gifts to your friends specification material carbon fiber color black handlebar clamp diameter mm length package included x cycling carbon fiber rise')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "with open(\"data/bike-items.txt\") as f:\n",
    "    r = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    rgx = re.compile(r'\\b[a-zA-Z]+\\b') \n",
    "    docs = [ (' '.join(re.findall(rgx, x[0])).lower(), ' '.join(re.findall(rgx, x[1])).lower())  \\\n",
    "                for i,x in enumerate(r) if i > 1 ]\n",
    "    \n",
    "print(docs[0][0],docs[0][1])\n",
    "\n",
    "items_t = [ d[0] for d in docs ] # item titles\n",
    "items_d = [ d[1] for d in docs ] # item descriptions\n",
    "items_i = range(0, len(items_t)) # item id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Set 1 - Term Frequency\n",
    "\n",
    "Let's start with the first 10 item titles from our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cycling bicycle mtb bike fixie gloss carbon fiber riser bar handlebar', 'bicycle rims x red speed internal hub wheel set beach cruiser bike', 'mavic crossride mountain bike wheels and wtb weirwolf tires', 'new kcnc arrow alloy stem black', 'rotor qxl aero oval road chainring']\n"
     ]
    }
   ],
   "source": [
    "corpus = items_t[0:5]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by computing the frequency of terms in the *entire* corpus.  We will do this by enumerating over the corpus of documents, tokenizing the documents and count the frequency of tokens.   The easiest way is to build a python dictionary where the key is the token and the value is the count.  You can review python dictionaries in the [docs](https://docs.python.org/2/tutorial/datastructures.html).\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Here is a part completed code snippet to compute term frequency.  \n",
    "Complete this code to correctly populate the term frequency dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'set': 1, 'bicycle': 2, 'cruiser': 1, 'tires': 1, 'fixie': 1, 'oval': 1, 'speed': 1, 'internal': 1, 'mountain': 1, 'cycling': 1, 'handlebar': 1, 'gloss': 1, 'chainring': 1, 'bike': 3, 'black': 1, 'new': 1, 'beach': 1, 'red': 1, 'kcnc': 1, 'wheel': 1, 'rotor': 1, 'fiber': 1, 'hub': 1, 'rims': 1, 'mavic': 1, 'aero': 1, 'stem': 1, 'alloy': 1, 'wtb': 1, 'carbon': 1, 'riser': 1, 'bar': 1, 'qxl': 1, 'crossride': 1, 'arrow': 1, 'weirwolf': 1, 'mtb': 1, 'x': 1, 'wheels': 1, 'road': 1}\n"
     ]
    }
   ],
   "source": [
    "tf = {}\n",
    "for doc in corpus:\n",
    "    for word in doc.split():\n",
    "        # << COMPUTE ERM FREQUENCY DICTIONARY >> CODE HERE\n",
    "        ## HIDE\n",
    "        if word in tf:\n",
    "            tf[word] += 1\n",
    "        else:\n",
    "            tf[word] = 1\n",
    "        ## HIDE\n",
    "\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify by using a [Counter](https://docs.python.org/2/library/collections.html#collections.Counter) rather than a dictionary.\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'> \n",
    "Take a look at the docs for the `Counter` collection.  \n",
    "Complete this function definition to compute term frequency using the `Counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'bike': 3, 'bicycle': 2, 'and': 1, 'set': 1, 'cruiser': 1, 'tires': 1, 'fixie': 1, 'oval': 1, 'speed': 1, 'internal': 1, 'mountain': 1, 'cycling': 1, 'handlebar': 1, 'gloss': 1, 'chainring': 1, 'black': 1, 'new': 1, 'beach': 1, 'red': 1, 'kcnc': 1, 'wheel': 1, 'rotor': 1, 'fiber': 1, 'hub': 1, 'rims': 1, 'mavic': 1, 'aero': 1, 'stem': 1, 'alloy': 1, 'wtb': 1, 'carbon': 1, 'riser': 1, 'bar': 1, 'qxl': 1, 'crossride': 1, 'arrow': 1, 'weirwolf': 1, 'mtb': 1, 'x': 1, 'wheels': 1, 'road': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_tf(corpus):\n",
    "    tf = Counter()\n",
    "    for doc in corpus:\n",
    "        for word in doc.split():\n",
    "            # << CODE HERE\n",
    "            ## HIDE\n",
    "            tf[word] += 1\n",
    "    return tf\n",
    "\n",
    "tf = get_tf(corpus)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Counter does not give us a real speed advantage - since it does more work.   For these tiny data sets we do not see any difference - however in Python 3 it is faster than a default dictionary.  Often times best way to test performance is to time code execution.\n",
    "\n",
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'> \n",
    "We should get used to thinking about performance.   \n",
    "We can use the Jupyter Notebook [magics](http://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb) to time the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Run the code to compute the term frequency for the full corpus of item titles.  \n",
    "What is the frequency of the terms 'unicycle', 'bicycle' and 'tricycle'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3544\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "tf = get_tf(items_t)\n",
    "\n",
    "# Print tf for 'unicycle'\n",
    "## HIDE\n",
    "print(tf['unicycle'])\n",
    "print(tf['bicycle'])\n",
    "print(tf['tricycle'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term frequency can also be computed for each document - the term frequency is a crude measure of the \"aboutness\" of a document.  For short documents, such as eBay item titles, terms do not occur very frequently.  In longer documents the term frequency is a form of compression and summarization.\n",
    "\n",
    "We can store the document term frequency in a dictionary, where the key is the document id and the value is the a nested dictionary of document terms and their counts.\n",
    "\n",
    "For example consider the corpus of three documents:\n",
    "\n",
    "1. 'mountain bike red'\n",
    "2. 'road bike carbon'\n",
    "3. 'bike helmet'\n",
    "\n",
    "The document term frequencies would be:\n",
    "\n",
    "| id | document term frequencies |\n",
    "|----|------------------|\n",
    "| 1  | { 'mountain' : 1, 'bike' : 1, 'red' : 1 } |\n",
    "| 2  | { 'road' : 1, 'bike' : 1, 'carbon' : 1 } |\n",
    "| 3  | { 'bike' : 1, 'helmet' : 1 } |\n",
    "\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Now compute **document term frequencies** for the full corpus of item titles.  \n",
    "Print out the document term frequencies for 3 randomly selected documents - what was the highest frequency term for each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'black': 1,\n",
       " 'blue': 1,\n",
       " 'giro': 1,\n",
       " 'ii': 1,\n",
       " 'milky': 1,\n",
       " 'nib': 1,\n",
       " 's': 1,\n",
       " 'sante': 1,\n",
       " 'shoes': 1,\n",
       " 'white': 1,\n",
       " 'women': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tfd(corpus):\n",
    "    tfd = {}\n",
    "    for i,doc in enumerate(corpus):\n",
    "        tfd[i]={}\n",
    "        # << DOCUMENT TERM FREQUENCY >> CODE HERE\n",
    "        ## HIDE\n",
    "        for word in doc.split():\n",
    "            if word in tfd[i]:\n",
    "                tfd[i][word] += 1\n",
    "            else:\n",
    "                tfd[i][word] = 1\n",
    "    return tfd\n",
    "            \n",
    "    \n",
    "tfd = get_tfd(items_t)\n",
    "tfd[234]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Repeat this time computing the **document term frequencies** for the full corpus of item descriptions.  \n",
    "Print out the document term frequency for 3 randomly selected documents - what was the highest frequency term for each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accurately': 2,\n",
       " 'also': 1,\n",
       " 'and': 1,\n",
       " 'answer': 1,\n",
       " 'answered': 1,\n",
       " 'are': 1,\n",
       " 'as': 4,\n",
       " 'ask': 2,\n",
       " 'at': 1,\n",
       " 'best': 1,\n",
       " 'black': 1,\n",
       " 'blue': 1,\n",
       " 'but': 1,\n",
       " 'buying': 1,\n",
       " 'can': 1,\n",
       " 'clothing': 1,\n",
       " 'describe': 1,\n",
       " 'descriptions': 1,\n",
       " 'experience': 1,\n",
       " 'feel': 2,\n",
       " 'for': 1,\n",
       " 'free': 2,\n",
       " 'giro': 1,\n",
       " 'have': 4,\n",
       " 'if': 3,\n",
       " 'ii': 1,\n",
       " 'in': 2,\n",
       " 'interested': 1,\n",
       " 'it': 1,\n",
       " 'items': 1,\n",
       " 'jae': 1,\n",
       " 'make': 1,\n",
       " 'may': 1,\n",
       " 'milky': 1,\n",
       " 'more': 1,\n",
       " 'nib': 1,\n",
       " 'not': 1,\n",
       " 'of': 1,\n",
       " 'one': 1,\n",
       " 'our': 3,\n",
       " 'please': 1,\n",
       " 'positive': 1,\n",
       " 'possible': 1,\n",
       " 'promptly': 1,\n",
       " 'questions': 2,\n",
       " 's': 1,\n",
       " 'sale': 1,\n",
       " 'sales': 1,\n",
       " 'sante': 1,\n",
       " 'shoes': 1,\n",
       " 'specific': 1,\n",
       " 'than': 1,\n",
       " 'them': 1,\n",
       " 'these': 1,\n",
       " 'to': 4,\n",
       " 'try': 2,\n",
       " 'we': 6,\n",
       " 'which': 1,\n",
       " 'white': 1,\n",
       " 'will': 1,\n",
       " 'women': 1,\n",
       " 'you': 3,\n",
       " 'your': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# << COMPUTE TFD FOR ITEM DESCRIPTIONS >> CODE HERE\n",
    "## HIDE\n",
    "tfd = get_tfd(items_d)\n",
    "tfd[234]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Set 2 - Term Frequency Ranking and Boolean Matching\n",
    "\n",
    "Whilst the document term frequency dictionary in the previous section `tfd` is a compact way to store the term frequency it is not efficient for analysis.  A term frequency matrix is a more effective way to store the data.  \n",
    "\n",
    "For example consider the corpus of three documents:\n",
    "\n",
    "1. 'mountain bike red'\n",
    "2. 'road bike carbon'\n",
    "3. 'bike helmet'\n",
    "\n",
    "There is a toal vocabulary of six terms [ 'mountain', 'bike' , 'red', 'road', 'carbon', 'helmet' ].\n",
    "\n",
    "Each document count be represented as a 3 x 6 element vectors:\n",
    "\n",
    "| Document ID | mountain | bike | red | road | carbon | helmet |\n",
    "|-------------|----------|------|-----|------|--------|--------|\n",
    "| 1 - 'mountain bike red' | 1 | 1 | 1 | 0 | 0 | 0 |\n",
    "| 2 - 'road bike carbon' | 0 | 1 | 0 | 1 | 1 | 0 |\n",
    "| 3 - 'bike helmet' | 0 | 1 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "Those arrays can be stacked naturally into a matrix - one row per document, one column per term.  We call this matrix the term frequency matrix.\n",
    "\n",
    "To compute the term frequency matrix we have to first compute the lexicon (set of terms) in our corpus.\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Review the docs for the [set](https://docs.python.org/2/library/stdtypes.html#set-types-set-frozenset) type. Note - sets do not contain duplicates and can be used to dedupe tokens.\n",
    "Complete the `get_lexicon()` function definition so that it returns a list of unique terms across a given corpus of documents.  Validate with the small test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_lexicon(corpus):\n",
    "    lexicon = set()\n",
    "    # << COMPUTE SET OF TERMS IN CORPUS >> CODE HERE\n",
    "    ## HIDE\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    ## HIDE\n",
    "    return list(lexicon)\n",
    "    \n",
    "test_corpus = ['mountain bike red','road bike carbon','bike helmet']\n",
    "lexicon = get_lexicon(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our lexicon we can compute a document term frequency matrix.  We will store our document term frequency vectors in a `list`.  Note we could also store them in a `dictionary` where the key is the document_id.  \n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Complete the code snippet below to compute the term frequency vector for each document.  \n",
    "Store the term frequency vectors in the list `tfm`.  Validate the results with the test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 1, 1, 0, 0], [0, 0, 1, 0, 1, 1], [0, 1, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "lexicon = get_lexicon(test_corpus)\n",
    "\n",
    "tfm =[]\n",
    "for doc in test_corpus:\n",
    "    tfv = [0]*len(lexicon)\n",
    "    for term in doc.split():\n",
    "        # << COMPUTE DOCUMENT TERM FREQUENCY VECTOR tfv AND APPEND TO tfm >> CODE HERE\n",
    "        ## HIDE\n",
    "        tfv[lexicon.index(term)] += 1\n",
    "        ## HIDE\n",
    "    tfm.append(tfv)\n",
    "    \n",
    "print(tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to reuse the tfm let's create a function that takes as argument the corpus and returns the lexicon and the tfm.\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Copy your code snippets from the previous two exercises into the function definition below.    \n",
    "Test your function by computing 'tfm' on the test corpus verifying your results before computing tfm for the item_titles corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tfm(corpus):\n",
    "    \n",
    "    def get_lexicon(corpus):\n",
    "        lexicon = set()\n",
    "        # << COMPUTE SET OF TERMS IN CORPUS >> CODE HERE\n",
    "        ## HIDE\n",
    "        for doc in corpus:\n",
    "            lexicon.update([word for word in doc.split()])\n",
    "        return list(lexicon)\n",
    "        ## HIDE\n",
    "        \n",
    "    lexicon = get_lexicon(corpus)\n",
    "\n",
    "    tfm =[]\n",
    "    for doc in corpus:\n",
    "        tfv = [0]*len(lexicon)\n",
    "        for term in doc.split():\n",
    "            # << COMPUTE DOCUMENT TERM FREQUENCY VECTOR AND APPEND TO tfm >> CODE HERE\n",
    "            ## HIDE\n",
    "            tfv[lexicon.index(term)] += 1\n",
    "            ## HIDE\n",
    "        tfm.append(tfv)\n",
    "        \n",
    "    return tfm, lexicon\n",
    "\n",
    "\n",
    "test_corpus = ['mountain bike red','road bike carbon','bike helmet']\n",
    "tfm, lexicon = get_tfm(test_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'> \n",
    "As our corpus increases so does the sparsity of the term frequency matrix - most elements have value zero.  \n",
    "We can use more efficient [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix) storage to save memory.  More details [here](http://localhost:8888/notebooks/1.3%20Introduction%20to%20Information%20Retrieval.ipynb#Sparse-Term-Frequency-Matrix).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Boolean Search\n",
    "\n",
    "We are now in a position to write our first ranking function.  Now we have the term frequency matrix we can use it to find documents that contain words included in a user specified query.  We will start by simply returning the documents from the corpus that match any terms in the query and rank by the raw frequency of matching terms. \n",
    "\n",
    "More specifically our algorithm for 'boolean search' proceeds as follows:\n",
    "\n",
    "1. Convert query to query vector using the lexicon for the corpus\n",
    "2. Compute a ranking score for each document by taking the [dot product](https://en.wikipedia.org/wiki/Dot_product) of the query vector and each document's term frequency vector\n",
    "3. Sort the documents by score\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "The function definion `get_results()` converts the user query `qry` into a vector using the supplied lexicon.\n",
    "Complete the function by providing the code to compute the score of each document.  Test using a bike related query such as 'led bike light'.  Do you get relevant results?  \n",
    "\n",
    "HINT : Here is a [gist](https://gist.github.com/mattwg/60910d90a8987e271212) that shows how to compute the dot product between two vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 from recall set of 9893 items ordered by tf-idf:\n",
      "\t6.00 - frog waterproof bike light set led white front light led red rear light\n",
      "\t4.00 - cycling bike bicycle led front light head light torch mount aaa\n",
      "\t4.00 - waterproof usb rechargeable led bike light set bright headlight free light\n",
      "\t4.00 - niterider tl sl led bike tail light red rear flashing bike safety\n",
      "\t4.00 - sets bright bike bicycle waterproof led head light led rearlight us seller\n",
      "\t4.00 - lm cree led cycling front bike bicycle light headlight only light\n",
      "\t4.00 - led tire valve stem caps neon light bike bicycle car auto wheel light\n",
      "\t4.00 - ultra bright waterproof silicon led bicycle light set led front rear light\n",
      "\t4.00 - usb cycling xml led front bike light bicycle light headlamp headlight\n",
      "\t4.00 - portable usb rechargeable bicycle bike tail rear light led safety warning light\n"
     ]
    }
   ],
   "source": [
    "def get_results(qry, tfm, lexicon):\n",
    "    qrv = [0]*len(lexicon)\n",
    "    for term in qry.split():\n",
    "        if term in lexicon:\n",
    "            qrv[lexicon.index(term)] = 1\n",
    "\n",
    "    results = []      \n",
    "    for i, tfv in enumerate(tfm):\n",
    "        score = 0\n",
    "        # << COMPUTE DOCUMENT SCORE >> CODE HERE\n",
    "        ## HIDE\n",
    "        score = sum([ xy[0] * xy[1] for xy in zip(qrv,tfv)])\n",
    "        ## HIDE\n",
    "        results.append([score, i])\n",
    "    \n",
    "    sorted_results = sorted(results, key=lambda t: t[0] * -1 )\n",
    "    return sorted_results\n",
    "\n",
    "\n",
    "def print_results(results,n, head=True):\n",
    "    ''' Helper function to print results\n",
    "    '''\n",
    "    if head:    \n",
    "        print('\\nTop %d from recall set of %d items ordered by tf-idf:' % (n,len(results)))\n",
    "        for r in results[:n]:\n",
    "            print('\\t%0.2f - %s'%(r[0],items_t[r[1]]))\n",
    "    else:\n",
    "        print('\\nBottom %d from recall set of %d items ordered by tf-idf:' % (n,len(results)))\n",
    "        for r in results[:n]:\n",
    "            print('\\t%0.2f - %s'%(r[0],items_t[r[1]]))\n",
    "    \n",
    "\n",
    "tfm, lexicon = get_tfm(items_t)\n",
    "results = get_results('led bike light', tfm , lexicon)\n",
    "print_results(results,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index\n",
    "\n",
    "This search across documents is expensive - especially if the score for many documents is zero!  To solve this problem we can create an inverted index.  An inverted index can be used to filter out documents that do not contain any of the keywords in the query before computing the ranking score.  \n",
    "\n",
    "Using our example mini-corpus:\n",
    "\n",
    "1. 'mountain bike red'\n",
    "2. 'road bike carbon'\n",
    "3. 'bike helmet'\n",
    "\n",
    "There is a toal vocabulary of six terms [ 'mountain', 'bike' , 'red', 'road', 'carbon', 'helmet' ].  An inverted index will map each of these terms to the document in which the document can be found.  \n",
    "\n",
    "| key | value |\n",
    "|-----|-------|\n",
    "| 'mountain' | [ 1 ] |  \n",
    "| 'bike' | [1, 2, 3] |  \n",
    "| 'red' | [1] |  \n",
    "| 'road' |  [2] | \n",
    "| 'carbon' | [2] | \n",
    "|  'helmet' | [3] |\n",
    "\n",
    "We could store an inverted index in a dictionary where the key is the term and the value is the document id.\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "We will create an inverted index as a python dictionary keyed on the token.  \n",
    "Complete the code snippet below to create the inverted index.  Validate with the test corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_inverted_index(corpus):\n",
    "    idx={}\n",
    "    for i, doc in enumerate(corpus):\n",
    "        # << POPULATE INVERTED INDEX >> CODE HERE\n",
    "        ## HIDE\n",
    "        for word in doc.split():\n",
    "            if word in idx:\n",
    "                idx[word].append(i)\n",
    "            else:\n",
    "                idx[word] = [i]\n",
    "        ## HIDE\n",
    "    return idx\n",
    "\n",
    "test_corpus = ['mountain bike red','road bike carbon','bike helmet']\n",
    "idx = create_inverted_index(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Now we can create an inverted index for all the item titles.  We can use the set intersection method to find all the documents that match the query 'led bike light'.  Run the code below checking the titles of some of the results that match query terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([9559, 2061, 2062, 8212, 31, 8229, 4134, 8238, 4143, 8248, 8250, 4159, 8258, 8261, 2118, 80, 4181, 8278, 91, 4193, 6244, 6247, 9237, 6282, 2195, 2196, 6297, 166, 4274, 2227, 6175, 2239, 4294, 4296, 6345, 4299, 4300, 8406, 8408, 6361, 2267, 2272, 8426, 4331, 4335, 6386, 6388, 245, 6407, 6417, 6419, 6424, 8474, 6434, 2339, 8485, 4398, 306, 8499, 4404, 4406, 316, 4413, 8515, 2372, 6469, 2375, 2387, 6497, 2402, 8553, 6514, 2451, 374, 8571, 384, 6533, 8583, 8590, 6544, 6547, 4501, 2459, 4512, 423, 8623, 4532, 2498, 8652, 4557, 8655, 472, 8665, 8670, 2529, 6629, 8681, 4592, 6230, 6662, 6664, 2569, 8715, 2589, 6686, 6235, 6694, 6697, 4658, 8756, 8761, 8763, 6733, 594, 8790, 599, 4699, 8798, 8807, 618, 8811, 4722, 6772, 2682, 2697, 2706, 4758, 4762, 4769, 4770, 2734, 6839, 2749, 8896, 2753, 2754, 8901, 6859, 717, 4817, 6867, 4821, 731, 6880, 6883, 8934, 4839, 4841, 9684, 8954, 4865, 773, 6918, 6920, 8970, 8972, 6926, 8978, 6934, 796, 8992, 4897, 2850, 9692, 6965, 6971, 4924, 829, 6974, 2901, 2903, 857, 4959, 867, 9060, 2922, 9073, 9022, 9087, 2944, 2948, 905, 2955, 5004, 7059, 9370, 7076, 935, 939, 9132, 941, 7087, 948, 5048, 9145, 3005, 5057, 9154, 5066, 9716, 5071, 5083, 5087, 9186, 2214, 5101, 9204, 9207, 3065, 9216, 9219, 9047, 5141, 7198, 1066, 9263, 9264, 5184, 1092, 9287, 5198, 9295, 9305, 9314, 7280, 9337, 9341, 7316, 1175, 1178, 1181, 7326, 5280, 3233, 9378, 9382, 3953, 7337, 6344, 7361, 1227, 8056, 9429, 1254, 5352, 3305, 1262, 5359, 5674, 9470, 9471, 9477, 9485, 1294, 9488, 7457, 8841, 9514, 9518, 5429, 9531, 3395, 7498, 7511, 3418, 9564, 1373, 5473, 3430, 7528, 3441, 9587, 5499, 9601, 9602, 3460, 9605, 9628, 9632, 9644, 5553, 9655, 9660, 7613, 9666, 9668, 7621, 1479, 9675, 5582, 3540, 3541, 3543, 9688, 7644, 5597, 9695, 3554, 1512, 1514, 5612, 5613, 7668, 9718, 1530, 7676, 1533, 5633, 9733, 7686, 1544, 3597, 3603, 9748, 1558, 3607, 9753, 3611, 7708, 9757, 9758, 1571, 9766, 4017, 9768, 9770, 1585, 9784, 5044, 7741, 5695, 9793, 9795, 5701, 7758, 7437, 9808, 1617, 9813, 9815, 3672, 9817, 7772, 9835, 5742, 957, 9841, 9842, 7806, 9858, 1668, 9861, 5771, 9870, 9873, 3694, 9879, 1688, 9881, 9839, 7837, 5797, 7858, 7864, 8820, 9162, 5822, 7881, 7889, 7893, 1752, 5856, 7911, 7912, 3822, 9854, 1783, 5881, 7938, 7956, 9860, 1821, 3870, 3874, 5933, 7983, 5937, 1847, 7135, 5951, 5962, 1873, 8026, 1884, 3938, 3609, 8041, 3947, 1905, 6008, 3963, 1922, 3971, 3977, 1936, 3736, 6036, 9883, 4004, 2033, 6065, 8116, 8117, 8118, 9546, 6079, 4033, 4034, 1989, 6087, 8142, 4054, 4057, 2010, 6068, 2016, 4081, 8188])\n",
      "universal bike bicycle led light flashlight torch lamp mount clamp stand holder\n"
     ]
    }
   ],
   "source": [
    "idx = create_inverted_index(items_t)\n",
    "print(set(idx['led']).intersection(set(idx['bike'])).intersection(set(idx['light'])))\n",
    "print(items_t[2061])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now improve on our first ranking function.  This time only scoring the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_results2(qry, idx):\n",
    "\n",
    "    score = Counter()\n",
    "    terms = qry.split()\n",
    "    for term in terms:\n",
    "        for doc in idx[term]:\n",
    "            score[doc] += 1\n",
    "            \n",
    "    results=[]\n",
    "    for x in [[r[0],r[1]] for r in zip(score.keys(), score.values())]:\n",
    "        if x[1] > 0:\n",
    "            # output [0] score, [1] doc_id\n",
    "            results.append([x[1],x[0]])\n",
    "\n",
    "    return results;\n",
    "\n",
    "\n",
    "idx = create_inverted_index(items_t)\n",
    "%timeit results = get_results2('front rear back led bike light', idx)\n",
    "\n",
    "print_results(results,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a lot of documents in the recall set since many match on one of the words - bike is present in almost every other document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'term':[x for x in idx.keys()],'freq':[len(x) for x in idx.values()]})\n",
    "\n",
    "output_notebook(hide_banner=True)\n",
    "p = Bar(df.sort_values('freq', ascending=False)[:30], label=CatAttr(columns=['term'], sort=False), values='freq',\n",
    "        plot_width=800, plot_height=400)\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency (IDF)\n",
    "\n",
    "It would seem sensible to down weight words that are very common in the corpus - the word 'bike' in a query is not as discriminating as the word 'front'. IDF is a way to quantify how common or rare a term is in the corpus.  It is computed by taking the log of the inverse fraction of the number of documents in which the term appears divided by the total number of documents.  To avoid division by zero it is common to add 1 to the number of documents in which the term appears.  \n",
    "\n",
    "IDF is already partially computed when we constructed the inverted index - it is the number of documents the term apears in - in otherwords the length of the document list in the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_inverted_index(corpus):\n",
    "    idx={}\n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word in doc.split():\n",
    "            if word in idx:\n",
    "                if i in idx[word]:\n",
    "                    # Update document's frequency\n",
    "                    idx[word][i] += 1\n",
    "                else:\n",
    "                    # Add document\n",
    "                    idx[word][i] = 1\n",
    "            else:\n",
    "                # Add term\n",
    "                idx[word] = {i:1}\n",
    "    return idx\n",
    "\n",
    "def idf(term, idx, n):\n",
    "    return math.log( float(n) / (1 + len(idx[term])))\n",
    "\n",
    "idx = create_inverted_index(items_t)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'term':[x for x in idx.keys()],'freq':[len(x) for x in idx.values()],\n",
    "                  'idf':[idf(x, idx, len(items_t)) for x in idx.keys()]})\n",
    "\n",
    "output_notebook(hide_banner=True)\n",
    "p1 = Bar(df.sort_values('freq', ascending=False)[:30], label=CatAttr(columns=['term'], sort=False), values='freq',\n",
    "        plot_width=800, plot_height=400)\n",
    "\n",
    "p2 = Bar(df.sort_values('freq', ascending=False)[:30], label=CatAttr(columns=['term'], sort=False), values='idf',\n",
    "        plot_width=800, plot_height=400)\n",
    "\n",
    "p = vplot(p1, p2)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking by TF-IDF\n",
    "\n",
    "We can now combine term frequency and inverse document frequency when computing the score for each item in the recall set.  Until now we have just computed the score as the raw frequency of the query terms in each document.  Now we want to weight the raw frequency by the inverse documetn frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results3(qry, idx, n):\n",
    "    score = Counter()\n",
    "    for term in qry.split():\n",
    "        if term in idf:\n",
    "            i = idf(term, idx, n)\n",
    "            for doc in idx[term]:\n",
    "                score[doc] += idx[term][doc] * i\n",
    "        \n",
    "    results=[]\n",
    "    for x in [[r[0],r[1]] for r in zip(score.keys(), score.values())]:\n",
    "        if x[1] > 0:\n",
    "            # output [0] score, [1] doc_id\n",
    "            results.append([x[1],x[0]])\n",
    "\n",
    "    return results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = create_inverted_index(items_t)\n",
    "results = get_results3('front led bike light', idx, len(items_t))\n",
    "\n",
    "print_results(results,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problematic queries!\n",
    "\n",
    "With this corpus we cannot search for mountain bikes without returning a heap of accesories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = create_inverted_index(items_t)\n",
    "results = get_results3('mountain bike', idx, len(items_t))\n",
    "\n",
    "print_results(results,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to penalise items where there are many more terms in the query.  For example the terms \"mountain\" and \"bike\" only make up 2 / 12 terms in the \"oakley mens automatic mountain mtb factory lite mountain bmx bike gloves large\" yet it scores highly because there is no penalty for all the other terms in the item title.\n",
    "\n",
    "In addition this scheme create discrete levels based on combination of word frequency:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'score':[float(x[0]) for x in results],\n",
    "                   'title':[items_t[x[1]] for x in results]})\n",
    "\n",
    "d = df.groupby('score').first().reset_index()\n",
    "\n",
    "r1 = re.compile('(bike)')\n",
    "r2 = re.compile('(mountain)')\n",
    "\n",
    "for i, t in enumerate(d.title):\n",
    "    n1 = r1.findall(t)\n",
    "    n2 = r2.findall(t)\n",
    "    print('%d x Bike, %d x Mountain, Score = %0.2f'%(len(n1),len(n2),d.score[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot score vs item length\n",
    "df = pd.DataFrame({'score':[float(x[0]) for x in results],\n",
    "                   'length':[len(items_t[x[1]].split()) for x in results]})\n",
    "\n",
    "output_notebook(hide_banner=True)\n",
    "p = Scatter(df, x='score', y='length')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we do not want scores to be the same for lots of documents.  We could try by boosting the score for documents that are shorter than average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results4(qry, corpus):\n",
    "    idx = create_inverted_index(corpus)\n",
    "    n = len(corpus)\n",
    "    d = [len(x.split()) for x in corpus]\n",
    "    d_avg = float(sum(d)) / len(d)\n",
    "    score = Counter()\n",
    "    for term in qry.split():\n",
    "        if term in idf:\n",
    "            i = idf(term, idx, n)\n",
    "            for doc in idx[term]:\n",
    "                f = float(idx[term][doc])\n",
    "                score[doc] += i *  ( f / (float(d[doc]) / d_avg) )\n",
    "        \n",
    "    results=[]\n",
    "    for x in [[r[0],r[1]] for r in zip(score.keys(), score.values())]:\n",
    "        if x[1] > 0:\n",
    "            # output [0] score, [1] doc_id\n",
    "            results.append([x[1],x[0]])\n",
    "\n",
    "    return results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = get_results4('mountain bike', items_t)\n",
    "print_results(results, 10)\n",
    "\n",
    "# Plot score vs item length\n",
    "df = pd.DataFrame({'score':[float(x[0]) for x in results],\n",
    "                   'length':[len(items_t[x[1]].split()) for x in results]})\n",
    "\n",
    "output_notebook()\n",
    "p = Scatter(df, x='score', y='length')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okapi BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results5(qry, corpus, k1=1.5, b=0.75):\n",
    "    idx = create_inverted_index(corpus)\n",
    "    n = len(corpus)\n",
    "    d = [len(x.split()) for x in corpus]\n",
    "    d_avg = float(sum(d)) / len(d)                \n",
    "    score = Counter()\n",
    "    for term in qry.split():\n",
    "        if term in idf:\n",
    "            i = idf(term, idx, n)\n",
    "            for doc in idx[term]:\n",
    "                f = float(idx[term][doc])\n",
    "                score[doc] += i * (( f * (k1 + 1) ) / (f + k1 * (1 - b + (b * (float(d[doc]) / d_avg)))))\n",
    "        \n",
    "    results=[]\n",
    "    for x in [[r[0],r[1]] for r in zip(score.keys(), score.values())]:\n",
    "        if x[1] > 0:\n",
    "            # output [0] score, [1] doc_id\n",
    "            results.append([x[1],x[0]])\n",
    "\n",
    "    return results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = get_results5('mountain bike', items_t)\n",
    "print_results(results, 10)\n",
    "\n",
    "# Plot score vs item length\n",
    "df = pd.DataFrame({'score':[float(x[0]) for x in results],\n",
    "                   'length':[len(items_t[x[1]].split()) for x in results]})\n",
    "\n",
    "output_notebook()\n",
    "p = Scatter(df, x='score', y='length')\n",
    "show(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Can we make the above interactive?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
