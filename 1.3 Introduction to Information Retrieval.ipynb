{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Introduction to Information Retrieval\n",
    "\n",
    "Here we work with an eBay item data set.  The data contains 9895 item titles and descriptions.\n",
    "\n",
    "First we load the data - this is easiest with a `csv.reader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a list of (item title, description) tuple :\n",
      " + cycling bicycle mtb bike fixie gloss carbon fiber riser bar handlebar\n",
      " + description feature easy to use made of high quality carbon fiber with the special design can save for a long time the carbon fiber handlebar is made of high quality carbon fiber so that you can use it relieved this quick disassembling carbon fiber handlebar is easy to use and one of the best gifts to your friends specification material carbon fiber color black handlebar clamp diameter mm length package included x cycling carbon fiber rise\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9893"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "with open(\"data/bike-items.txt\") as f:\n",
    "    r = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    rgx = re.compile(r'\\b[a-zA-Z]+\\b') \n",
    "    docs = [ (' '.join(re.findall(rgx, x[0])).lower(), ' '.join(re.findall(rgx, x[1])).lower())  for i,x in enumerate(r) if i > 1 ]\n",
    "\n",
    "print('We have a list of (item title, description) tuple :\\n + %s\\n + %s' % (docs[0][0],docs[0][1]))\n",
    "\n",
    "items_t = [ d[0] for d in docs ] # item titles\n",
    "items_d = [ d[1] for d in docs ] # item descriptions\n",
    "items_i = range(0, len(items_t)) # item id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcount across all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5803\n",
      "1068\n",
      "5803\n",
      "1068\n"
     ]
    }
   ],
   "source": [
    "# Without counters\n",
    "wc = {}\n",
    "for d in items_d:\n",
    "    for w in d.split(' '):\n",
    "        if w in wc:\n",
    "            wc[w] += 1\n",
    "        else:\n",
    "            wc[w] = 1\n",
    "\n",
    "print(wc['bike'])\n",
    "print(wc['carbon'])\n",
    "\n",
    "# With counters\n",
    "wc = Counter()\n",
    "for d in items_d:\n",
    "    for w in d.split(' '):\n",
    "        wc[w] += 1\n",
    "        \n",
    "print(wc['bike'])\n",
    "print(wc['carbon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency matrix\n",
    "\n",
    "We can start by creating a document by document word count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 5, 'to': 4, 'paypal': 3, 'only': 2, 'please': 2, 'destinations': 2, 'ship': 2, 'shipping': 2, 'address': 2, 'in': 2, 'your': 2, 'set': 1, 'checkout': 1, 'wheel': 1, 'payment': 1, 'rates': 1, 'listed': 1, 'item': 1, 'is': 1, 'clyde': 1, 'us': 1, 'time': 1, 'quote': 1, 'of': 1, 'account': 1, 'united': 1, 'before': 1, 'states': 1, 'are': 1, 'threw': 1, 'other': 1, 'speed': 1, 'a': 1, 'message': 1, 'within': 1, 'hub': 1, 'orders': 1, 'commercial': 1, 'verify': 1, 'or': 1, 'we': 1, 'most': 1, 'residential': 1, 'red': 1, 'send': 1, 'cycles': 1, 'at': 1, 'correct': 1, 'verified': 1, 'that': 1, 'after': 1, 'james': 1, 'continental': 1, 'sent': 1, 'receiving': 1, 'x': 1, 'for': 1, 'internal': 1, 'days': 1, 'payme': 1, 'making': 1})\n"
     ]
    }
   ],
   "source": [
    "tf = {}\n",
    "for i, d in enumerate(items_d):\n",
    "    dtf = Counter()\n",
    "    for w in d.split(' '):\n",
    "            dtf[w] += 1\n",
    "    tf[i] = dtf\n",
    "        \n",
    "print(tf[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally our term-frequency is a matrix not a ragged array!  Each document should be a vector that has an element for each document in the corpus.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1], [1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "def get_lexicon(corpus):\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return lexicon\n",
    "\n",
    "corpus = ['the quick brown fox','mary had a little lamb','the owl and the pussycat']\n",
    "lexicon = get_lexicon(corpus)\n",
    "\n",
    "tfm =[]\n",
    "for doc in corpus:\n",
    "    for term in doc.split():\n",
    "        tfv = [doc.split().count(word) for word in lexicon]\n",
    "    tfm.append(tfv)\n",
    "        \n",
    "print(tfm)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As number of terms increases this method becomes inefficient.  Here is a faster implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1], [1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "def get_lexicon(corpus):\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return list(lexicon)\n",
    "\n",
    "corpus = ['the quick brown fox','mary had a little lamb','the owl and the pussycat']\n",
    "lexicon = get_lexicon(corpus)\n",
    "\n",
    "tfm =[]\n",
    "for doc in corpus:\n",
    "    tfv = [0]*len(lexicon)\n",
    "    for term in doc.split():\n",
    "        tfv[lexicon.index(term)] += 1\n",
    "    tfm.append(tfv)\n",
    "        \n",
    "print(tfm)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare the time of each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 83.2 µs per loop\n",
      "100000 loops, best of 3: 11.1 µs per loop\n"
     ]
    }
   ],
   "source": [
    "def tfm1(corpus):\n",
    "    \n",
    "    def get_lexicon(corpus):\n",
    "        lexicon = set()\n",
    "        for doc in corpus:\n",
    "            lexicon.update([word for word in doc.split()])\n",
    "        return lexicon\n",
    "    \n",
    "    lexicon = get_lexicon(corpus)\n",
    "\n",
    "    tfm =[]\n",
    "    for doc in corpus:\n",
    "        for term in doc.split():\n",
    "            tfv = [doc.split().count(word) for word in lexicon]\n",
    "        tfm.append(tfv)\n",
    "    \n",
    "    return tfm\n",
    "\n",
    "def tfm2(corpus):\n",
    "    \n",
    "    def get_lexicon(corpus):\n",
    "        lexicon = set()\n",
    "        for doc in corpus:\n",
    "            lexicon.update([word for word in doc.split()])\n",
    "        return list(lexicon)\n",
    "\n",
    "    lexicon = get_lexicon(corpus)\n",
    "\n",
    "    tfm =[]\n",
    "    for doc in corpus:\n",
    "        tfv = [0]*len(lexicon)\n",
    "        for term in doc.split():\n",
    "            tfv[lexicon.index(term)] += 1\n",
    "        tfm.append(tfv)\n",
    "    \n",
    "    return tfm\n",
    "\n",
    "corpus = ['the quick brown fox','mary had a little lamb','the owl and the pussycat']\n",
    "\n",
    "%timeit tfm1(corpus)\n",
    "%timeit tfm2(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfm = tfm2(items_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9893 15841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15791"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tfm), len(tfm[0]))\n",
    "tfm[0].count(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have high percentage of zero elements - we should really find better way to store the tf matrix.  How about numpy "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
